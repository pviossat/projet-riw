{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de RIW\n",
    "\n",
    "Par Antoine Apollis, Marine Sobas et Paul Viossat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/marine/.local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/marine/.local/lib/python3.7/site-packages (from nltk) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/marine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions régulières et constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "punctuation_regex = compile(r'[,.;:?!—–&]?[\" ]+|[\"\\']')\n",
    "number_regex = compile(\"^[0-9,.]*$\")\n",
    "index_regex = compile(r'(\\S+), \\d+ \\| (\\(\\S+, \\d+ ;.*)')\n",
    "doc_occ_pos_regex = compile(r'\\((\\S+), (\\d+) ; ((?:\\d+ ?)+)\\) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILENAME = 'INDEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = {2: 'stanford class', 3: 'stanford students', 4: 'very cool', 8: 'stanford computer science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_LENGTH = {2: 6094, 3: 22335, 4: 63, 8: 4232}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions de traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizes a character string\n",
    "def tokenize(s):\n",
    "    return [w.lower() for w in punctuation_regex.split(s) if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Removes stop words (from NLTK) from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(array):\n",
    "    [w for w in filtered_sentence if not number_regex.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizes a list of tokens\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stems a list of tokens\n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de l’index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an index in a file\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILENAME, 'w') as f:\n",
    "        for word in index:\n",
    "            f.write(f'{word}, {len(index[word])} | ')\n",
    "            for (document, tokens) in index[word].items():\n",
    "                f.write(f'({document}, {tokens[0]} ; {\" \".join(map(str, tokens[1]))}) ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an index from a file\n",
    "def load_index():\n",
    "    with open(INDEX_FILENAME) as f:\n",
    "        inverted_index = dict()\n",
    "        for l in f:\n",
    "            m = index_regex.match(l)\n",
    "            inverted_index[m.group(1)] = dict(map(lambda t: (t[0], [int(t[1]), list(map(int, t[2].split(' ')))]), doc_occ_pos_regex.findall(m.group(2))))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary from a (tokenized) colelction\n",
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for tokens in collection.values():\n",
    "        for t in tokens:\n",
    "            vocabulary.add(t)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a document from the disk\n",
    "def load_document(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os import listdir\n",
    "COLLECTION_FILENAME = \"collection\"\n",
    "\n",
    "# Loads (and tokenizes) a collection from a directory\n",
    "def load_collection(directory,COLLECTION_FILENAME,forcebuild=False):\n",
    "    if not forcebuild and isfile(COLLECTION_FILENAME):\n",
    "        filehandler = open(COLLECTION_FILENAME, 'rb')\n",
    "        collection=  pickle.load(filehandler)\n",
    "    else:\n",
    "        print('Chargement de la collection : ', end='')\n",
    "        collection = dict()\n",
    "        for sub_dir in listdir(directory):\n",
    "            if sub_dir != \".DS_Store\":\n",
    "                print(sub_dir)\n",
    "                path = directory + '/' + sub_dir\n",
    "                for filename in listdir(path):\n",
    "                    fullpath = './' + path + '/' + filename\n",
    "                    collection[fullpath] = list()\n",
    "        print('fait')\n",
    "        ndocuments = len(collection)\n",
    "        print(f'La collection comporte {ndocuments} documents.\\n======')\n",
    "        progress = 0\n",
    "        step = ndocuments // 10\n",
    "        nextstep = step\n",
    "        chrono = time()\n",
    "        for fullpath in collection.keys():\n",
    "            collection[fullpath] = stem(remove_stop_words(tokenize(load_document(fullpath))))\n",
    "            progress += 1\n",
    "            if progress > nextstep:\n",
    "                print(f'Traitement de la collection en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress) / 60)} min')\n",
    "                nextstep += step\n",
    "        filehander = open('collection', 'wb') \n",
    "        pickle.dump(collection, filehander)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an inverted index from a collection and a vocabulary\n",
    "def build_index(collection, vocabulary):\n",
    "    index = {word: dict() for word in vocabulary}\n",
    "    progress = 0\n",
    "    ndocuments = len(collection)\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for (document, tokens) in collection.items():\n",
    "        i = 0\n",
    "        for t in tokens:\n",
    "            if document in index[t]:\n",
    "                index[t][document][0] += 1\n",
    "                index[t][document][1].append(i)\n",
    "            else:\n",
    "                index[t][document] = [1, [i]]\n",
    "            i += 1\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Création de l’index en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress))} s')\n",
    "            nextstep += step\n",
    "    save_index(index)\n",
    "    print('\\nIndex créé et enregistré')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os.path import isfile, getsize\n",
    "\n",
    "# Builds an inverted index for a given directory\n",
    "def build_inverted_index(directory, forcebuild=False):\n",
    "    if not forcebuild and isfile(INDEX_FILENAME):\n",
    "        return load_index()\n",
    "    fullchrono = time()\n",
    "    collection = load_collection(directory)\n",
    "    print('======\\nCréation du vocabulaire : ', end='')\n",
    "    vocabulary = extract_vocabulary(collection)\n",
    "    print('fait')\n",
    "    print(f'Le vocabulaire comporte {len(vocabulary)} éléments.\\n======')\n",
    "    index = build_index(collection, vocabulary)\n",
    "    print(f'======\\nL’opération complète a nécessité {(time() - fullchrono) / 60:.1f} minutes.')\n",
    "    print(f'L’index occupe {getsize(INDEX_FILENAME) // 1000} ko.')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_inverted_index('pa1-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche booléenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a boolean AND search query\n",
    "def boolean_search(query):\n",
    "    # We stem all the terms of the query.\n",
    "    terms = list(map(lambda t: stem([t])[0], query.split(' ')))\n",
    "    if len(terms) < 1:\n",
    "        return set()\n",
    "    results = set(index[terms[0]].keys())\n",
    "    for t in terms[1:]:\n",
    "        results &= set(index[t].keys())\n",
    "    return sorted(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: retrieved 35.59% of results in excess\n",
      "Query 3: retrieved 25.12% of results in excess\n",
      "Query 4: missed 100.00% of the results\n",
      "Query 8: retrieved 74.93% of results in excess\n"
     ]
    }
   ],
   "source": [
    "for i, q in QUERIES.items():\n",
    "    r = len(boolean_search(q))\n",
    "    if r < RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: missed {100 * (RESULTS_LENGTH[i] - r) / RESULTS_LENGTH[i]:.2f}% of the results')\n",
    "    elif r > RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: retrieved {100 * (r - RESULTS_LENGTH[i]) / RESULTS_LENGTH[i]:.2f}% of results in excess')\n",
    "    else:\n",
    "        print(f'Query {i}: retrieved the correct number of results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriétés de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "collection = load_collection('pa1-data',COLLECTION_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18244627\n"
     ]
    }
   ],
   "source": [
    "v = sum(map(len, collection.values()))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection comporte donc 98 998 documents, et 18 244 627 mots (après retrait des mots vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296631\n"
     ]
    }
   ],
   "source": [
    "m = len(index.keys())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vocabulaire comporte 162 076 mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modèle vectoriel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4778/98998 [00:00<00:01, 47775.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics on the entire collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98998/98998 [00:02<00:00, 44643.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from vectorial_model import processing_vectorial_query,get_stats_collection\n",
    "\n",
    "weighting_schemes = {\"frequency\":\"FRQ\",\"tf_idf_normalize\":\"TIN\",\"tf_idf_logarithmic\":\"TIL\",\"tf_idf_logarithmic_normalize\":\"TILN\",\"binary\":\"BIN\"}\n",
    "q = list(QUERIES.values())[-1]\n",
    "stats_collection = get_stats_collection(collection)\n",
    "weighting_scheme_query = weighting_schemes[\"frequency\"]\n",
    "weighting_scheme_document = weighting_schemes[\"tf_idf_logarithmic_normalize\"]\n",
    "processed = processing_vectorial_query(q, index, stats_collection, weighting_scheme_document,weighting_scheme_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc_ids = {}\n",
    "\n",
    "for model in weighting_schemes.keys():\n",
    "    relevant_doc_ids[model] = {}\n",
    "    for i,query in QUERIES.items() :\n",
    "        relevant_doc_ids[model][str(i)] = list(processing_vectorial_query(query, index, stats_collection, weighting_schemes[model],weighting_scheme_query).keys())\n",
    "        \n",
    "relevant_doc_ids[\"boolean\"] = {}\n",
    "for i,query in QUERIES.items():\n",
    "    relevant_doc_ids[\"boolean\"][str(i)] = boolean_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1624/73196 [00:00<00:09, 7645.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for frequency\n",
      "Computing the mean average precision for query stanford class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73196/73196 [07:45<00:00, 157.24it/s]\n",
      "  2%|▏         | 1200/73002 [00:00<00:05, 11991.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for query stanford students\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73002/73002 [12:45<00:00, 95.41it/s]  \n",
      "100%|██████████| 779/779 [00:00<00:00, 39714.64it/s]\n",
      "  2%|▏         | 1216/74739 [00:00<00:06, 12156.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74739/74739 [06:37<00:00, 187.82it/s] \n",
      "  2%|▏         | 1646/73196 [00:00<00:09, 7635.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for tf_idf_normalize\n",
      "Computing the mean average precision for query stanford class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73196/73196 [06:26<00:00, 189.42it/s]\n",
      "  2%|▏         | 1664/73002 [00:00<00:09, 7764.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for query stanford students\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73002/73002 [12:12<00:00, 99.66it/s] \n",
      "100%|██████████| 779/779 [00:00<00:00, 43870.17it/s]\n",
      "  2%|▏         | 1250/74739 [00:00<00:05, 12478.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74739/74739 [06:38<00:00, 187.77it/s] \n",
      "  3%|▎         | 1578/61431 [00:00<00:08, 7186.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for tf_idf_logarithmic\n",
      "Computing the mean average precision for query stanford class\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61431/61431 [04:23<00:00, 233.50it/s]\n",
      "  2%|▏         | 1157/61606 [00:00<00:05, 11568.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for query stanford students\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 9427/61606 [00:09<01:47, 486.43it/s]  "
     ]
    }
   ],
   "source": [
    "from utils import get_queries_output\n",
    "from eval import mAp\n",
    "\n",
    "true_results= get_queries_output(directory='Queries/dev_output')\n",
    "maps = {}\n",
    "models =  list(relevant_doc_ids.keys())\n",
    "for model in models:\n",
    "    print(\"Computing the mean average precision for {}\".format(model))\n",
    "    obtained_results = relevant_doc_ids[model]\n",
    "    maps[model] = mAp(QUERIES,obtained_results,true_results, 10,model,forcebuild= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}