{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de RIW\n",
    "\n",
    "Par Antoine Apollis, Marine Sobas et Paul Viossat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/paul/.local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/paul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions régulières et constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "punctuation_regex = compile(r'[,.;:?!—–&]?[\" ]+|[\"\\']')\n",
    "number_regex = compile(\"^[0-9,.]*$\")\n",
    "index_regex = compile(r'(\\S+), \\d+ \\| (\\(\\S+, \\d+ ;.*)')\n",
    "doc_occ_pos_regex = compile(r'\\((\\S+), (\\d+) ; ((?:\\d+ ?)+)\\) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILENAME = 'INDEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = {2: 'stanford class', 3: 'stanford students', 4: 'very cool', 8: 'stanford computer science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_LENGTH = {2: 6094, 3: 22335, 4: 63, 8: 4232}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions de traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizes a character string\n",
    "def tokenize(s):\n",
    "    return [w.lower() for w in punctuation_regex.split(s) if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Removes stop words (from NLTK) from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(array):\n",
    "    [w for w in filtered_sentence if not number_regex.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizes a list of tokens\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stems a list of tokens\n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de l’index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an index in a file\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILENAME, 'w') as f:\n",
    "        for word in index:\n",
    "            f.write(f'{word}, {len(index[word])} | ')\n",
    "            for (document, tokens) in index[word].items():\n",
    "                f.write(f'({document}, {tokens[0]} ; {\" \".join(map(str, tokens[1]))}) ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an index from a file\n",
    "def load_index():\n",
    "    with open(INDEX_FILENAME) as f:\n",
    "        inverted_index = dict()\n",
    "        for l in f:\n",
    "            m = index_regex.match(l)\n",
    "            inverted_index[m.group(1)] = dict(map(lambda t: (t[0], [int(t[1]), list(map(int, t[2].split(' ')))]), doc_occ_pos_regex.findall(m.group(2))))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary from a (tokenized) colelction\n",
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for tokens in collection.values():\n",
    "        for t in tokens:\n",
    "            vocabulary.add(t)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a document from the disk\n",
    "def load_document(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os import listdir\n",
    "COLLECTION_FILENAME = \"collection\"\n",
    "\n",
    "# Loads (and tokenizes) a collection from a directory\n",
    "def load_collection(directory,COLLECTION_FILENAME,forcebuild=False):\n",
    "    if not forcebuild and isfile(COLLECTION_FILENAME):\n",
    "        filehandler = open(COLLECTION_FILENAME, 'rb')\n",
    "        collection=  pickle.load(filehandler)\n",
    "    else:\n",
    "        print('Chargement de la collection : ', end='')\n",
    "        collection = dict()\n",
    "        for sub_dir in listdir(directory):\n",
    "            if sub_dir != \".DS_Store\":\n",
    "                print(sub_dir)\n",
    "                path = directory + '/' + sub_dir\n",
    "                for filename in listdir(path):\n",
    "                    fullpath = './' + path + '/' + filename\n",
    "                    collection[fullpath] = list()\n",
    "        print('fait')\n",
    "        ndocuments = len(collection)\n",
    "        print(f'La collection comporte {ndocuments} documents.\\n======')\n",
    "        progress = 0\n",
    "        step = ndocuments // 10\n",
    "        nextstep = step\n",
    "        chrono = time()\n",
    "        for fullpath in collection.keys():\n",
    "            collection[fullpath] = stem(remove_stop_words(tokenize(load_document(fullpath))))\n",
    "            progress += 1\n",
    "            if progress > nextstep:\n",
    "                print(f'Traitement de la collection en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress) / 60)} min')\n",
    "                nextstep += step\n",
    "        filehander = open('collection', 'wb') \n",
    "        pickle.dump(collection, filehander)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an inverted index from a collection and a vocabulary\n",
    "def build_index(collection, vocabulary):\n",
    "    index = {word: dict() for word in vocabulary}\n",
    "    progress = 0\n",
    "    ndocuments = len(collection)\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for (document, tokens) in collection.items():\n",
    "        i = 0\n",
    "        for t in tokens:\n",
    "            if document in index[t]:\n",
    "                index[t][document][0] += 1\n",
    "                index[t][document][1].append(i)\n",
    "            else:\n",
    "                index[t][document] = [1, [i]]\n",
    "            i += 1\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Création de l’index en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress))} s')\n",
    "            nextstep += step\n",
    "    save_index(index)\n",
    "    print('\\nIndex créé et enregistré')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os.path import isfile, getsize\n",
    "\n",
    "# Builds an inverted index for a given directory\n",
    "def build_inverted_index(directory, forcebuild=False):\n",
    "    if not forcebuild and isfile(INDEX_FILENAME):\n",
    "        return load_index()\n",
    "    fullchrono = time()\n",
    "    collection = load_collection(directory)\n",
    "    print('======\\nCréation du vocabulaire : ', end='')\n",
    "    vocabulary = extract_vocabulary(collection)\n",
    "    print('fait')\n",
    "    print(f'Le vocabulaire comporte {len(vocabulary)} éléments.\\n======')\n",
    "    index = build_index(collection, vocabulary)\n",
    "    print(f'======\\nL’opération complète a nécessité {(time() - fullchrono) / 60:.1f} minutes.')\n",
    "    print(f'L’index occupe {getsize(INDEX_FILENAME) // 1000} ko.')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_collection() missing 1 required positional argument: 'COLLECTION_FILENAME'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a25b48b65564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_inverted_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pa1-data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1abb02f2efde>\u001b[0m in \u001b[0;36mbuild_inverted_index\u001b[0;34m(directory, forcebuild)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfullchrono\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======\\nCréation du vocabulaire : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_collection() missing 1 required positional argument: 'COLLECTION_FILENAME'"
     ]
    }
   ],
   "source": [
    "index = build_inverted_index('pa1-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche booléenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a boolean AND search query\n",
    "def boolean_search(query):\n",
    "    # We stem all the terms of the query.\n",
    "    terms = list(map(lambda t: stem([t])[0], query.split(' ')))\n",
    "    if len(terms) < 1:\n",
    "        return set()\n",
    "    results = set(index[terms[0]].keys())\n",
    "    for t in terms[1:]:\n",
    "        results &= set(index[t].keys())\n",
    "    return sorted(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, q in QUERIES.items():\n",
    "    r = len(boolean_search(q))\n",
    "    if r < RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: missed {100 * (RESULTS_LENGTH[i] - r) / RESULTS_LENGTH[i]:.2f}% of the results')\n",
    "    elif r > RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: retrieved {100 * (r - RESULTS_LENGTH[i]) / RESULTS_LENGTH[i]:.2f}% of results in excess')\n",
    "    else:\n",
    "        print(f'Query {i}: retrieved the correct number of results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriétés de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "collection = load_collection('pa1-data',COLLECTION_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18244627\n"
     ]
    }
   ],
   "source": [
    "v = sum(map(len, collection.values()))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection comporte donc 98 998 documents, et 18 244 627 mots (après retrait des mots vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-a59cb0cc9872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "m = len(index.keys())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Le vocabulaire comporte 162 076 mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modèle vectoriel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3929/98998 [00:00<00:02, 39287.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics on the entire collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98998/98998 [00:02<00:00, 35492.12it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-3aac831e74cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mweighting_scheme_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frequency\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mweighting_scheme_document\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tf_idf_logarithmic_normalize\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_vectorial_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting_scheme_document\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweighting_scheme_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "from vectorial_model import processing_vectorial_query,get_stats_collection\n",
    "\n",
    "weighting_schemes = {\"frequency\":\"FRQ\",\"tf_idf_normalize\":\"TIN\",\"tf_idf_logarithmic\":\"TIL\",\"tf_idf_logarithmic_normalize\":\"TILN\",\"binary\":\"BIN\"}\n",
    "q = list(QUERIES.values())[-1]\n",
    "stats_collection = get_stats_collection(collection)\n",
    "weighting_scheme_query = weighting_schemes[\"frequency\"]\n",
    "weighting_scheme_document = weighting_schemes[\"tf_idf_logarithmic_normalize\"]\n",
    "processed = processing_vectorial_query(q, index, stats_collection, weighting_scheme_document,weighting_scheme_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc_ids = {}\n",
    "\n",
    "for model in weighting_schemes.keys():\n",
    "    relevant_doc_ids[model] = {}\n",
    "    for i,query in QUERIES.items() :\n",
    "        relevant_doc_ids[model][str(i)] = list(processing_vectorial_query(query, index, stats_collection, weighting_schemes[model],weighting_scheme_query).keys())\n",
    "        \n",
    "relevant_doc_ids[\"boolean\"] = {}\n",
    "for i,query in QUERIES.items():\n",
    "    relevant_doc_ids[\"boolean\"][str(i)] = boolean_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import get_queries_output\n",
    "from eval import mAp\n",
    "\n",
    "true_results= get_queries_output(directory='Queries/dev_output')\n",
    "maps = {}\n",
    "models =  list(relevant_doc_ids.keys())\n",
    "for model in models:\n",
    "    print(\"Computing the mean average precision for {}\".format(model))\n",
    "    obtained_results = relevant_doc_ids[model]\n",
    "    maps[model] = mAp(QUERIES,obtained_results,true_results, 10,model,forcebuild= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./pa1-data/4/nifty.stanford.edu_2007_clancy-slidingblocks_easy_immed.match.2%2b270', 0.019925153854885604)\n"
     ]
    }
   ],
   "source": [
    "from langage_model import create_vocabulary, create_language_model, rank_document\n",
    "\n",
    "language_model_vocab = create_vocabulary(collection)\n",
    "language_model = create_language_model(collection, language_model_vocab, 1)\n",
    "\n",
    "for i, query in QUERIES.items():\n",
    "    rank_document(query, language_model, language_model_vocab, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
