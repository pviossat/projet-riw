{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de RIW\n",
    "\n",
    "Par Antoine Apollis, Marine Sobas et Paul Viossat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/paul/.local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/marine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Please refer the following variables with your names and custom directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILENAME = 'stored_objects/INDEX'\n",
    "DATA_DIRECTORY = data_directory = 'pa1-data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions régulières et constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "punctuation_regex = compile(r'[,.;:?!—–&]?[\" ]+|[\"\\']')\n",
    "number_regex = compile(\"^[0-9,.]*$\")\n",
    "index_regex = compile(r'(\\S+), \\d+ \\| (\\(\\S+, \\d+ ;.*)')\n",
    "doc_occ_pos_regex = compile(r'\\((\\S+), (\\d+) ; ((?:\\d+ ?)+)\\) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = {2: 'stanford class', 3: 'stanford students', 4: 'very cool', 8: 'stanford computer science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_LENGTH = {2: 6094, 3: 22335, 4: 63, 8: 4232}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions de traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizes a character string\n",
    "def tokenize(s):\n",
    "    return [w.lower() for w in punctuation_regex.split(s) if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Removes stop words (from NLTK) from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(array):\n",
    "    [w for w in filtered_sentence if not number_regex.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizes a list of tokens\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stems a list of tokens\n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de l’index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an index in a file\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILENAME, 'w') as f:\n",
    "        for word in index:\n",
    "            f.write(f'{word}, {len(index[word])} | ')\n",
    "            for (document, tokens) in index[word].items():\n",
    "                f.write(f'({document}, {tokens[0]} ; {\" \".join(map(str, tokens[1]))}) ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an index from a file\n",
    "def load_index():\n",
    "    with open(INDEX_FILENAME) as f:\n",
    "        inverted_index = dict()\n",
    "        for l in f:\n",
    "            m = index_regex.match(l)\n",
    "            inverted_index[m.group(1)] = dict(map(lambda t: (t[0], [int(t[1]), list(map(int, t[2].split(' ')))]), doc_occ_pos_regex.findall(m.group(2))))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary from a (tokenized) colelction\n",
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for tokens in collection.values():\n",
    "        for t in tokens:\n",
    "            vocabulary.add(t)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a document from the disk\n",
    "def load_document(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "# Loads (and tokenizes) a collection from a directory\n",
    "def load_collection(directory,COLLECTION_FILENAME= \"stored_objects/collection\",forcebuild=False):\n",
    "    if not forcebuild and isfile(COLLECTION_FILENAME):\n",
    "        filehandler = open(COLLECTION_FILENAME, 'rb')\n",
    "        collection=  pickle.load(filehandler)\n",
    "    else:\n",
    "        print('Chargement de la collection : ', end='')\n",
    "        collection = dict()\n",
    "        for sub_dir in listdir(directory):\n",
    "            if sub_dir != \".DS_Store\":\n",
    "                print(sub_dir)\n",
    "                path = directory + '/' + sub_dir\n",
    "                for filename in listdir(path):\n",
    "                    fullpath = './' + path + '/' + filename\n",
    "                    collection[fullpath] = list()\n",
    "        print('fait')\n",
    "        ndocuments = len(collection)\n",
    "        print(f'La collection comporte {ndocuments} documents.\\n======')\n",
    "        progress = 0\n",
    "        step = ndocuments // 10\n",
    "        nextstep = step\n",
    "        chrono = time()\n",
    "        for fullpath in collection.keys():\n",
    "            collection[fullpath] = stem(remove_stop_words(tokenize(load_document(fullpath))))\n",
    "            progress += 1\n",
    "            if progress > nextstep:\n",
    "                print(f'Traitement de la collection en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress) / 60)} min')\n",
    "                nextstep += step\n",
    "        filehander = open(COLLECTION_FILENAME, 'wb') \n",
    "        pickle.dump(collection, filehander)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an inverted index from a collection and a vocabulary\n",
    "def build_index(collection, vocabulary):\n",
    "    index = {word: dict() for word in vocabulary}\n",
    "    progress = 0\n",
    "    ndocuments = len(collection)\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for (document, tokens) in collection.items():\n",
    "        i = 0\n",
    "        for t in tokens:\n",
    "            if document in index[t]:\n",
    "                index[t][document][0] += 1\n",
    "                index[t][document][1].append(i)\n",
    "            else:\n",
    "                index[t][document] = [1, [i]]\n",
    "            i += 1\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Création de l’index en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress))} s')\n",
    "            nextstep += step\n",
    "    save_index(index)\n",
    "    print('\\nIndex créé et enregistré')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os.path import isfile, getsize\n",
    "\n",
    "# Builds an inverted index for a given directory\n",
    "def build_inverted_index(directory, forcebuild=False):\n",
    "    if not forcebuild and isfile(INDEX_FILENAME):\n",
    "        return load_index()\n",
    "    fullchrono = time()\n",
    "    collection = load_collection(directory)\n",
    "    print('======\\nCréation du vocabulaire : ', end='')\n",
    "    vocabulary = extract_vocabulary(collection)\n",
    "    print('fait')\n",
    "    print(f'Le vocabulaire comporte {len(vocabulary)} éléments.\\n======')\n",
    "    index = build_index(collection, vocabulary)\n",
    "    print(f'======\\nL’opération complète a nécessité {(time() - fullchrono) / 60:.1f} minutes.')\n",
    "    print(f'L’index occupe {getsize(INDEX_FILENAME) // 1000} ko.')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_inverted_index(DATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche booléenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a boolean AND search query\n",
    "def boolean_search(query):\n",
    "    # We stem all the terms of the query.\n",
    "    terms = list(map(lambda t: stem([t])[0], query.split(' ')))\n",
    "    if len(terms) < 1:\n",
    "        return set()\n",
    "    results = set(index[terms[0]].keys())\n",
    "    for t in terms[1:]:\n",
    "        results &= set(index[t].keys())\n",
    "    return sorted(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: retrieved 35.59% of results in excess\n",
      "Query 3: retrieved 25.12% of results in excess\n",
      "Query 4: missed 100.00% of the results\n",
      "Query 8: retrieved 74.93% of results in excess\n"
     ]
    }
   ],
   "source": [
    "for i, q in QUERIES.items():\n",
    "    r = len(boolean_search(q))\n",
    "    if r < RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: missed {100 * (RESULTS_LENGTH[i] - r) / RESULTS_LENGTH[i]:.2f}% of the results')\n",
    "    elif r > RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: retrieved {100 * (r - RESULTS_LENGTH[i]) / RESULTS_LENGTH[i]:.2f}% of results in excess')\n",
    "    else:\n",
    "        print(f'Query {i}: retrieved the correct number of results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriétés de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de la collection : 9\n",
      "0\n",
      "7\n",
      "6\n",
      "1\n",
      "8\n",
      "4\n",
      "3\n",
      "2\n",
      "5\n",
      "fait\n",
      "La collection comporte 98998 documents.\n",
      "======\n",
      "Traitement de la collection en cours : encore 5 min\n",
      "Traitement de la collection en cours : encore 5 min\n",
      "Traitement de la collection en cours : encore 4 min\n",
      "Traitement de la collection en cours : encore 4 min\n",
      "Traitement de la collection en cours : encore 3 min\n",
      "Traitement de la collection en cours : encore 3 min\n",
      "Traitement de la collection en cours : encore 2 min\n",
      "Traitement de la collection en cours : encore 1 min\n",
      "Traitement de la collection en cours : encore 1 min\n",
      "Traitement de la collection en cours : encore 0 min\n"
     ]
    }
   ],
   "source": [
    "collection = load_collection(data_directory,COLLECTION_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18244627\n"
     ]
    }
   ],
   "source": [
    "v = sum(map(len, collection.values()))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection comporte donc 98 998 documents, et 18 244 627 mots (après retrait des mots vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296631\n"
     ]
    }
   ],
   "source": [
    "m = len(index.keys())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vocabulaire comporte 162 076 mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modèle vectoriel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different weighting procedures were considered. We will eventually compare them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5342/98998 [00:00<00:01, 53419.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics on the entire collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98998/98998 [00:01<00:00, 53559.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from models.vectorial_model import processing_vectorial_query,get_stats_collection\n",
    "\n",
    "weighting_schemes = {\"frequency\":\"FRQ\",\"tf_idf_normalize\":\"TIN\",\"tf_idf_logarithmic\":\"TIL\",\"tf_idf_logarithmic_normalize\":\"TILN\",\"binary\":\"BIN\"}\n",
    "q = list(QUERIES.values())[-1]\n",
    "stats_collection = get_stats_collection(collection)\n",
    "weighting_scheme_query = weighting_schemes[\"frequency\"]\n",
    "weighting_scheme_document = weighting_schemes[\"tf_idf_logarithmic_normalize\"]\n",
    "processed = processing_vectorial_query(q, index, stats_collection, weighting_scheme_document,weighting_scheme_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de langue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from models.langage_model import create_vocabulary, create_language_model, rank_document\n",
    "\n",
    "language_model_vocab = create_vocabulary(collection)\n",
    "language_model = create_language_model(collection, language_model_vocab, 1)\n",
    "\n",
    "for i, query in QUERIES.items():\n",
    "    rank_document(query, language_model, language_model_vocab, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate several models, we first perform a search on the different queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_doc_ids = {}\n",
    "\n",
    "for model in weighting_schemes.keys():\n",
    "    relevant_doc_ids[model] = {}\n",
    "    for i,query in QUERIES.items() :\n",
    "        relevant_doc_ids[model][str(i)] = list(processing_vectorial_query(query, index, stats_collection, weighting_schemes[model],weighting_scheme_query).keys())\n",
    "        \n",
    "relevant_doc_ids[\"boolean\"] = {}\n",
    "for i,query in QUERIES.items():\n",
    "    relevant_doc_ids[\"boolean\"][str(i)] = boolean_search(query)\n",
    "\n",
    "relevant_doc_ids[\"language_model\"] = {}\n",
    "for i, query in QUERIES.items():\n",
    "    relevant_doc_ids[\"language_model\"][str(i)] = list(zip(*rank_document(query, language_model, language_model_vocab, 1)))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The dictionary maps contains the mean average precisions over recall 11 values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the mean average precision for frequency\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for tf_idf_normalize\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for tf_idf_logarithmic\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for tf_idf_logarithmic_normalize\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for binary\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for boolean\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n",
      "Computing the mean average precision for language_model\n",
      "Computing the mean average precision for query stanford class\n",
      "Computing the mean average precision for query stanford students\n",
      "Computing the mean average precision for query very cool\n",
      "Computing the mean average precision for query stanford computer science\n"
     ]
    }
   ],
   "source": [
    "from helpers.utils import get_queries_output\n",
    "from helpers.eval import mAp\n",
    "\n",
    "true_results= get_queries_output(directory='Queries/dev_output')\n",
    "maps = {}\n",
    "models =  list(relevant_doc_ids.keys())\n",
    "for model in models:\n",
    "    print(\"Computing the mean average precision for {}\".format(model))\n",
    "    obtained_results = relevant_doc_ids[model]\n",
    "    maps[model] = mAp(QUERIES,obtained_results,true_results, 10,model,forcebuild= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 0.5682991168121805,\n",
       " 'tf_idf_normalize': 0.5568345204655083,\n",
       " 'tf_idf_logarithmic': 0.3132045670003387,\n",
       " 'tf_idf_logarithmic_normalize': 0.5816251247698814,\n",
       " 'binary': 0.5513329440214542,\n",
       " 'boolean': 0.5908503184476259,\n",
       " 'language_model': 0.16539163866684028}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}