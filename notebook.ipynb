{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de RIW\n",
    "\n",
    "Par Antoine Apollis, Marine Sobas et Paul Viossat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/marine/.local/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Users/marine/.local/lib/python3.7/site-packages (from nltk) (1.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/marine/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions régulières et constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "punctuation_regex = compile(r'[,.;:?!—–&]?[\" ]+|[\"\\']')\n",
    "number_regex = compile(\"^[0-9,.]*$\")\n",
    "index_regex = compile(r'(\\S+), \\d+ \\| (\\(\\S+, \\d+ ;.*)')\n",
    "doc_occ_pos_regex = compile(r'\\((\\S+), (\\d+) ; ((?:\\d+ ?)+)\\) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILENAME = 'INDEX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERIES = {2: 'stanford class', 3: 'stanford students', 4: 'very cool', 8: 'stanford computer science'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_LENGTH = {2: 6094, 3: 22335, 4: 63, 8: 4232}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions de traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizes a character string\n",
    "def tokenize(s):\n",
    "    return [w.lower() for w in punctuation_regex.split(s) if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Removes stop words (from NLTK) from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(array):\n",
    "    [w for w in filtered_sentence if not number_regex.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizes a list of tokens\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stems a list of tokens\n",
    "def stem(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de l’index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an index in a file\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILENAME, 'w') as f:\n",
    "        for word in index:\n",
    "            f.write(f'{word}, {len(index[word])} | ')\n",
    "            for (document, tokens) in index[word].items():\n",
    "                f.write(f'({document}, {tokens[0]} ; {\" \".join(map(str, tokens[1]))}) ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an index from a file\n",
    "def load_index():\n",
    "    with open(INDEX_FILENAME) as f:\n",
    "        inverted_index = dict()\n",
    "        for l in f:\n",
    "            m = index_regex.match(l)\n",
    "            inverted_index[m.group(1)] = dict(map(lambda t: (t[0], [int(t[1]), list(map(int, t[2].split(' ')))]), doc_occ_pos_regex.findall(m.group(2))))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary from a (tokenized) colelction\n",
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for tokens in collection.values():\n",
    "        for t in tokens:\n",
    "            vocabulary.add(t)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a document from the disk\n",
    "def load_document(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os import listdir\n",
    "COLLECTION_FILENAME = \"collection\"\n",
    "\n",
    "# Loads (and tokenizes) a collection from a directory\n",
    "def load_collection(directory,COLLECTION_FILENAME,forcebuild=False):\n",
    "    if not forcebuild and isfile(COLLECTION_FILENAME):\n",
    "        filehandler = open(COLLECTION_FILENAME, 'rb')\n",
    "        collection=  pickle.load(filehandler)\n",
    "    else:\n",
    "        print('Chargement de la collection : ', end='')\n",
    "        collection = dict()\n",
    "        for sub_dir in listdir(directory):\n",
    "            if sub_dir != \".DS_Store\":\n",
    "                print(sub_dir)\n",
    "                path = directory + '/' + sub_dir\n",
    "                for filename in listdir(path):\n",
    "                    fullpath = './' + path + '/' + filename\n",
    "                    collection[fullpath] = list()\n",
    "        print('fait')\n",
    "        ndocuments = len(collection)\n",
    "        print(f'La collection comporte {ndocuments} documents.\\n======')\n",
    "        progress = 0\n",
    "        step = ndocuments // 10\n",
    "        nextstep = step\n",
    "        chrono = time()\n",
    "        for fullpath in collection.keys():\n",
    "            collection[fullpath] = stem(remove_stop_words(tokenize(load_document(fullpath))))\n",
    "            progress += 1\n",
    "            if progress > nextstep:\n",
    "                print(f'Traitement de la collection en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress) / 60)} min')\n",
    "                nextstep += step\n",
    "        filehander = open('collection', 'wb') \n",
    "        pickle.dump(collection, filehander)\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an inverted index from a collection and a vocabulary\n",
    "def build_index(collection, vocabulary):\n",
    "    index = {word: dict() for word in vocabulary}\n",
    "    progress = 0\n",
    "    ndocuments = len(collection)\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for (document, tokens) in collection.items():\n",
    "        i = 0\n",
    "        for t in tokens:\n",
    "            if document in index[t]:\n",
    "                index[t][document][0] += 1\n",
    "                index[t][document][1].append(i)\n",
    "            else:\n",
    "                index[t][document] = [1, [i]]\n",
    "            i += 1\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Création de l’index en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress))} s')\n",
    "            nextstep += step\n",
    "    save_index(index)\n",
    "    print('\\nIndex créé et enregistré')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os.path import isfile, getsize\n",
    "\n",
    "# Builds an inverted index for a given directory\n",
    "def build_inverted_index(directory, forcebuild=False):\n",
    "    if not forcebuild and isfile(INDEX_FILENAME):\n",
    "        return load_index()\n",
    "    fullchrono = time()\n",
    "    collection = load_collection(directory)\n",
    "    print('======\\nCréation du vocabulaire : ', end='')\n",
    "    vocabulary = extract_vocabulary(collection)\n",
    "    print('fait')\n",
    "    print(f'Le vocabulaire comporte {len(vocabulary)} éléments.\\n======')\n",
    "    index = build_index(collection, vocabulary)\n",
    "    print(f'======\\nL’opération complète a nécessité {(time() - fullchrono) / 60:.1f} minutes.')\n",
    "    print(f'L’index occupe {getsize(INDEX_FILENAME) // 1000} ko.')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_inverted_index('pa1-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recherche booléenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs a boolean AND search query\n",
    "def boolean_search(query):\n",
    "    # We stem all the terms of the query.\n",
    "    terms = list(map(lambda t: stem([t])[0], query.split(' ')))\n",
    "    if len(terms) < 1:\n",
    "        return set()\n",
    "    results = set(index[terms[0]].keys())\n",
    "    for t in terms[1:]:\n",
    "        results &= set(index[t].keys())\n",
    "    return sorted(list(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: retrieved 35.59% of results in excess\n",
      "Query 3: retrieved 25.12% of results in excess\n",
      "Query 4: missed 100.00% of the results\n",
      "Query 8: retrieved 74.93% of results in excess\n"
     ]
    }
   ],
   "source": [
    "for i, q in QUERIES.items():\n",
    "    r = len(boolean_search(q))\n",
    "    if r < RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: missed {100 * (RESULTS_LENGTH[i] - r) / RESULTS_LENGTH[i]:.2f}% of the results')\n",
    "    elif r > RESULTS_LENGTH[i]:\n",
    "        print(f'Query {i}: retrieved {100 * (r - RESULTS_LENGTH[i]) / RESULTS_LENGTH[i]:.2f}% of results in excess')\n",
    "    else:\n",
    "        print(f'Query {i}: retrieved the correct number of results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriétés de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "collection = load_collection('pa1-data',COLLECTION_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18244627\n"
     ]
    }
   ],
   "source": [
    "v = sum(map(len, collection.values()))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection comporte donc 98 998 documents, et 18 244 627 mots (après retrait des mots vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296631\n"
     ]
    }
   ],
   "source": [
    "m = len(index.keys())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vocabulaire comporte 162 076 mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modèle vectoriel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4524/98998 [00:00<00:02, 44335.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing statistics on the entire collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98998/98998 [00:01<00:00, 50926.31it/s]\n"
     ]
    }
   ],
   "source": [
    "from vectorial_model import processing_vectorial_query,get_stats_collection\n",
    "weighting_schemes = {\"frequency\":\"FRQ\",\"tf_idf_normalize\":\"TIN\",\"tf_idf_logarithmic\":\"TIL\",\"tf_idf_logarithmic_normalize\":\"TILN\"}\n",
    "q = list(QUERIES.values())[-1]\n",
    "stats_collection = get_stats_collection(collection)\n",
    "weighting_scheme_query = weighting_schemes[\"frequency\"]\n",
    "weighting_scheme_document = weighting_schemes[\"tf_idf_logarithmic_normalize\"]\n",
    "processed = processing_vectorial_query(q, index, stats_collection, weighting_scheme_document,weighting_scheme_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des différentes pondérations pour le modèle vectoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'frequency': 'FRQ',\n",
       " 'tf_idf_normalize': 'TIN',\n",
       " 'tf_idf_logarithmic': 'TIL',\n",
       " 'tf_idf_logarithmic_normalize': 'TILN'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighting_schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 10355/73196 [00:12<02:25, 431.25it/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a57b9d78c535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQUERIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrue_results\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mquery_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcurves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_recall_precision_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforcebuild\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mdraw_recall_precision_curve\u001b[0;34m(query, index, model, true_result, weighting_schemes, stats_collection, forcebuild)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            weighting_scheme_query)\n\u001b[1;32m     36\u001b[0m         \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprecision_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfilehander\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilehander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            weighting_scheme_query)\n\u001b[1;32m     36\u001b[0m         \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprecision_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfilehander\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilehander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(doc_id, true_result, ordered_relevant_docs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mobtained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_relevant_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mfalse_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtained\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrue_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utils import get_queries_output\n",
    "from eval import draw_recall_precision_curve\n",
    "\n",
    "query_number = \"1\"\n",
    "query_output= get_queries_output(directory='Queries/dev_output')\n",
    "model = \"frequency\"\n",
    "curves = {}\n",
    "for i,query in QUERIES.items():\n",
    "    true_results =  query_output[str(i)] \n",
    "    curves[query] = draw_recall_precision_curve(query, index, model, true_results, weighting_schemes, stats_collection,forcebuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/73196 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1179/73196 [00:00<00:06, 11782.45it/s]\u001b[A\n",
      "  2%|▏         | 1592/73196 [00:00<00:09, 7572.08it/s] \u001b[A\n",
      "  3%|▎         | 1964/73196 [00:00<00:13, 5212.13it/s]\u001b[A\n",
      "  3%|▎         | 2315/73196 [00:00<00:18, 3875.66it/s]\u001b[A\n",
      "  4%|▎         | 2634/73196 [00:00<00:22, 3133.02it/s]\u001b[A\n",
      "  4%|▍         | 2922/73196 [00:00<00:27, 2586.71it/s]\u001b[A\n",
      "  4%|▍         | 3179/73196 [00:00<00:31, 2255.06it/s]\u001b[A\n",
      "  5%|▍         | 3411/73196 [00:01<00:34, 2000.78it/s]\u001b[A\n",
      "  5%|▍         | 3621/73196 [00:01<00:41, 1675.42it/s]\u001b[A\n",
      "  5%|▌         | 3805/73196 [00:01<00:45, 1514.87it/s]\u001b[A\n",
      "  5%|▌         | 3971/73196 [00:01<00:49, 1401.67it/s]\u001b[A\n",
      "  6%|▌         | 4123/73196 [00:01<00:53, 1302.94it/s]\u001b[A\n",
      "  6%|▌         | 4263/73196 [00:01<00:54, 1256.96it/s]\u001b[A\n",
      "  6%|▌         | 4396/73196 [00:01<00:56, 1228.49it/s]\u001b[A\n",
      "  6%|▌         | 4524/73196 [00:02<00:57, 1198.75it/s]\u001b[A\n",
      "  6%|▋         | 4648/73196 [00:02<01:00, 1134.35it/s]\u001b[A\n",
      "  7%|▋         | 4765/73196 [00:02<01:03, 1084.07it/s]\u001b[A\n",
      "  7%|▋         | 4877/73196 [00:02<01:07, 1019.49it/s]\u001b[A\n",
      "  7%|▋         | 4982/73196 [00:02<01:12, 939.06it/s] \u001b[A\n",
      "  7%|▋         | 5080/73196 [00:02<01:17, 883.11it/s]\u001b[A\n",
      "  7%|▋         | 5172/73196 [00:02<01:22, 829.38it/s]\u001b[A\n",
      "  7%|▋         | 5258/73196 [00:02<01:27, 777.15it/s]\u001b[A\n",
      "  7%|▋         | 5339/73196 [00:03<01:30, 750.96it/s]\u001b[A\n",
      "  7%|▋         | 5417/73196 [00:03<01:31, 738.31it/s]\u001b[A\n",
      "  8%|▊         | 5493/73196 [00:03<01:33, 721.66it/s]\u001b[A\n",
      "  8%|▊         | 5567/73196 [00:03<01:37, 694.00it/s]\u001b[A\n",
      "  8%|▊         | 5638/73196 [00:03<01:39, 678.65it/s]\u001b[A\n",
      "  8%|▊         | 5707/73196 [00:03<01:40, 668.64it/s]\u001b[A\n",
      " 14%|█▍        | 10355/73196 [00:30<02:25, 431.25it/s][A\n",
      "  8%|▊         | 5842/73196 [00:03<01:44, 645.06it/s]\u001b[A\n",
      "  8%|▊         | 5907/73196 [00:03<01:46, 630.16it/s]\u001b[A\n",
      "  8%|▊         | 5971/73196 [00:03<01:46, 630.49it/s]\u001b[A\n",
      "  8%|▊         | 6035/73196 [00:04<01:47, 624.82it/s]\u001b[A\n",
      "  8%|▊         | 6098/73196 [00:04<01:49, 611.13it/s]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-cac5fb76c3a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmean_average_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQUERIES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweighting_schemes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mmean_average_precision\u001b[0;34m(QUERIES, index, model, true_results, K, weighting_schemes, stats_collection, forcebuild)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0maverage_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQUERIES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0maverage_precision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforcebuild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mmean_precision\u001b[0;34m(query, index, model, true_result, K, weighting_schemes, stats_collection, forcebuild)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmean_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforcebuild\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_recall_precision_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting_schemes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_collection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforcebuild\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mdraw_recall_precision_curve\u001b[0;34m(query, index, model, true_result, weighting_schemes, stats_collection, forcebuild)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            weighting_scheme_query)\n\u001b[1;32m     36\u001b[0m         \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprecision_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfilehander\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilehander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     35\u001b[0m                                            weighting_scheme_query)\n\u001b[1;32m     36\u001b[0m         \u001b[0mrelevant_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprecision_recall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mfilehander\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilehander\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/Centrale/3A/RI/projet-riw/eval.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(doc_id, true_result, ordered_relevant_docs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mobtained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mordered_relevant_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mfalse_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtained\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mtrue_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobtained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_positives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_positives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from eval import mean_average_precision\n",
    "\n",
    "maps ={}\n",
    "for model in weighting_schemes.keys():\n",
    "    maps[model]  = mean_average_precision(QUERIES, index, model, query_output, 10,weighting_schemes,stats_collection,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
