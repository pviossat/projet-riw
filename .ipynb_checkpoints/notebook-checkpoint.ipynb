{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet de RIW\n",
    "\n",
    "Par Antoine Apollis, Marine Sobas et Paul Viossat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --user nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressions régulières et constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import compile\n",
    "\n",
    "punctuation_regex = compile(r'[,.;:?!—–&]?[\" ]+|[\"\\']')\n",
    "number_regex = compile(\"^[0-9,.]*$\")\n",
    "index_regex = compile(r'(\\S+), \\d+ \\| (\\(\\S+, \\d+ ;.*)')\n",
    "doc_occ_pos_regex = compile(r'\\((\\S+), (\\d+) ; ((?:\\d+ ?)+)\\) ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FILENAME = 'INDEX'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions de traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenizes a character string\n",
    "def tokenize(s):\n",
    "    return [w.lower() for w in punctuation_regex.split(s) if len(w) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# Removes stop words (from NLTK) from a list of tokens\n",
    "def remove_stop_words(tokens):\n",
    "    return [w for w in tokens if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(array):\n",
    "    [w for w in filtered_sentence if not number_regex.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizes a list of tokens\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(w) for w in tokens]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stems a list of tokens\n",
    "def stem(tokens):\n",
    "    return [stemmer.stem(w) for w in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de l’index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves an index in a file\n",
    "def save_index(index):\n",
    "    with open(INDEX_FILENAME, 'w') as f:\n",
    "        for word in index:\n",
    "            f.write(f'{word}, {len(index[word])} | ')\n",
    "            for (document, tokens) in index[word].items():\n",
    "                f.write(f'({document}, {tokens[0]} ; {\" \".join(map(str, tokens[1]))}) ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads an index from a file\n",
    "def load_index():\n",
    "    with open(INDEX_FILENAME) as f:\n",
    "        inverted_index = dict()\n",
    "        for l in f:\n",
    "            m = index_regex.match(l)\n",
    "            inverted_index[m.group(1)] = dict(map(lambda t: (t[0], [int(t[1]), list(map(int, t[2].split(' ')))]), doc_occ_pos_regex.findall(m.group(2))))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the vocabulary from a (tokenized) colelction\n",
    "def extract_vocabulary(collection):\n",
    "    vocabulary = set()\n",
    "    for tokens in collection.values():\n",
    "        for t in tokens:\n",
    "            vocabulary.add(t)\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a document from the disk\n",
    "def load_document(filename):\n",
    "    with open(filename) as f:\n",
    "        return f.read().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os import listdir\n",
    "\n",
    "# Loads (and tokenizes) a collection from a directory\n",
    "def load_collection(directory):\n",
    "    print('Chargement de la collection : ', end='')\n",
    "    collection = dict()\n",
    "    for sub_dir in listdir(directory):\n",
    "        path = directory + '/' + sub_dir\n",
    "        for filename in listdir(path):\n",
    "            fullpath = './' + path + '/' + filename\n",
    "            collection[fullpath] = list()\n",
    "    print('fait')\n",
    "    ndocuments = len(collection)\n",
    "    print(f'La collection comporte {ndocuments} documents.\\n======')\n",
    "    progress = 0\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for fullpath in collection.keys():\n",
    "        collection[fullpath] = stem(remove_stop_words(tokenize(load_document(fullpath))))\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Traitement de la collection en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress) / 60)} min')\n",
    "            nextstep += step\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds an inverted index from a collection and a vocabulary\n",
    "def build_index(collection, vocabulary):\n",
    "    index = {word: dict() for word in vocabulary}\n",
    "    progress = 0\n",
    "    ndocuments = len(collection)\n",
    "    step = ndocuments // 10\n",
    "    nextstep = step\n",
    "    chrono = time()\n",
    "    for (document, tokens) in collection.items():\n",
    "        i = 0\n",
    "        for t in tokens:\n",
    "            if document in index[t]:\n",
    "                index[t][document][0] += 1\n",
    "                index[t][document][1].append(i)\n",
    "            else:\n",
    "                index[t][document] = [1, [i]]\n",
    "            i += 1\n",
    "        progress += 1\n",
    "        if progress > nextstep:\n",
    "            print(f'Création de l’index en cours : encore {round((time() - chrono) / nextstep * (ndocuments - progress))} s')\n",
    "            nextstep += step\n",
    "    save_index(index)\n",
    "    print('\\nIndex créé et enregistré')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from os.path import isfile, getsize\n",
    "\n",
    "# Builds an inverted index for a given directory\n",
    "def build_inverted_index(directory, forcebuild=False):\n",
    "    if not forcebuild and isfile(INDEX_FILENAME):\n",
    "        return load_index()\n",
    "    fullchrono = time()\n",
    "    collection = load_collection(directory)\n",
    "    print('======\\nCréation du vocabulaire : ', end='')\n",
    "    vocabulary = extract_vocabulary(collection)\n",
    "    print('fait')\n",
    "    print(f'Le vocabulaire comporte {len(vocabulary)} éléments.\\n======')\n",
    "    index = build_index(collection, vocabulary)\n",
    "    print(f'======\\nL’opération complète a nécessité {(time() - fullchrono) / 60:.1f} minutes.')\n",
    "    print(f'L’index occupe {getsize(INDEX_FILENAME) // 1000} ko.')\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = build_inverted_index('pa1-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriétés de la collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = load_collection('pa1-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = sum(map(len, collection.values()))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La collection comporte donc 98 998 documents, et 18 244 627 mots (après retrait des mots vides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(index.keys())\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vocabulaire comporte 162 076 mots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
